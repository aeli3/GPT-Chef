{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5073a099-d720-4890-8401-e320e055e2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, GPT2Config\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model_name: ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']\n",
    "model_name = \"gpt2\" \n",
    "model_save_path = './model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73849abe-c73a-40f0-b8d7-02a374f6d289",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28d9cfc2-bc0c-4900-bd8a-19bea8b44295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beef, salt, pepper, garlic, parsley and a small amount of chili powder to taste. This is a recipe that we've been using at the grocery store for quite a while.\n",
      "\n",
      "If you'd like to make this recipe, please share on social media with the hashtag #yummytomato. I'd love to see what you have to share with us, please take a picture or post a link below.\n",
      "\n",
      "You'll also need a good quality blender, such as this one I use in the grocery store.\n",
      "\n",
      "4.0 from 5 votes Print Y\n",
      "beef, salt, pepper and olive oil.\n",
      "\n",
      "For this I used black olives and dried fruits. The olives were sweetened with anise.\n",
      "\n",
      "For this I used black beans and onions, fresh tomatoes, dried figs, basil, garlic and oregano. I then chopped the tomatoes into chunks and baked them in the oven for 45 minutes. Then, I baked the cooked tomatoes in the oven for another 20 minutes. Then I baked the roasted potatoes in the oven for another 15 minutes.\n",
      "\n",
      "For this I used white beans, dried fruit, parsley and\n",
      "beef, salt, pepper, garlic, pepper and vinegar, plus cayenne pepper.\n",
      "\n",
      "Then, sprinkle with the garlic and mix well. The whole thing is about to be done, but it looks like there are some important parts that you may not have considered before.\n",
      "\n",
      "What is a Cheese Dip?\n",
      "\n",
      "The term \"cheese dip\" describes a delicious, easy cheese dip. The term \"cheese dip\" is just a fancy word for an almost tasteless cheese dip. If you like to make something that tastes like a cheese dip, the simplest cheese dip can\n"
     ]
    }
   ],
   "source": [
    "configuration = GPT2Config.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, config=configuration)\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "input_sequence = \"beef, salt, pepper\"\n",
    "input_ids = tokenizer.encode(input_sequence, return_tensors='pt')\n",
    "\n",
    "model = model.to(device)\n",
    "#combine both sampling techniques\n",
    "sample_outputs = model.generate(input_ids.to(device),\n",
    "                              do_sample = True, max_length = 120,\n",
    "                              top_k = 50, top_p = 0.85,\n",
    "                              num_return_sequences = 3)\n",
    "\n",
    "for sample in sample_outputs:\n",
    "    print(tokenizer.decode(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55849c2-f46f-4560-a6f7-b0110aa2f4d4",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e609cba-fec3-4b5b-a252-1d30bf652528",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recipes = pd.read_csv('recipes_1000.csv')\n",
    "df_recipes.reset_index(drop=True, inplace=True)\n",
    "\n",
    "def form_string(ingredient,instruction):\n",
    "    s = f\"<|startoftext|>Ingredients: {ingredient.strip()}. \" \\\n",
    "        f\"Instructions: {instruction.strip()}<|endoftext|>\"\n",
    "    return s\n",
    "\n",
    "data = df_recipes.apply(lambda x:form_string(\n",
    "    x['ingredients'], x['instructions']), axis=1).to_list()\n",
    "data[0]\n",
    "\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_name,\n",
    "                                              bos_token='<|startoftext|>',\n",
    "                                              eos_token='<|endoftext|>',\n",
    "                                              unk_token='<|unknown|>',\n",
    "                                              pad_token='<|pad|>'\n",
    "                                             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a14044-8019-4cbe-83c5-9a9dc8a3d75c",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05723d72-26ad-4c40-8181-da503250405c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dataset[0]: \n",
      "  input_ids: tensor([50257, 41222, 25,  4171, 20853, 11, 19468, 4817,\n",
      "        16858, 32132, 11, 18873, 13135, 13, 27759, 25, 309, 793,\n",
      "        4691, 13, 50256, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
      "        50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259])\n",
      "   attn_masks: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "max_length = 180  \n",
    "\n",
    "# standard PyTorch approach of loading data in using a Dataset class.\n",
    "class RecipeDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "\n",
    "        for recipe in data:\n",
    "            encodings = tokenizer.encode_plus(recipe,\n",
    "                                              truncation=True,\n",
    "                                              padding='max_length',\n",
    "                                              max_length=max_length,\n",
    "                                              # return a PyTorch tensor\n",
    "                                              return_tensors='pt'       \n",
    "                                             )\n",
    "            self.input_ids.append(torch.squeeze(encodings['input_ids'],0))\n",
    "            self.attn_masks.append(torch.squeeze(encodings['attention_mask'],0))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]\n",
    "\n",
    "dataset = RecipeDataset(data, tokenizer)\n",
    "print(f\"input_ids: {dataset[0][0]} attn_masks: {dataset[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889f72b5-fe65-4f3c-bd64-68579b9a05b5",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69db5e9e-0f67-4e8d-a6dd-c2011cf8fca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create the DataLoaders for our training and validation datasets.\n",
    "# Get training samples in random order.\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset, \n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# Get valiation samples sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, \n",
    "            sampler = SequentialSampler(val_dataset),\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "\n",
    "configuration = GPT2Config.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, config=configuration)\n",
    "model = model.to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "epochs = 3\n",
    "learning_rate = 2e-5\n",
    "warmup_steps = 1e2\n",
    "# to prevent any division by zero in the implementation\n",
    "epsilon = 1e-8\n",
    "optim = AdamW(model.parameters(), lr = learning_rate, eps = epsilon)\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs  # [no batches] x [no epochs]\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optim,\n",
    "                                            num_warmup_steps=warmup_steps,\n",
    "                                            num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c47829-35ce-4997-80a4-c242a4bd48a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_i in range(0, epochs):\n",
    "    total_train_loss = 0\n",
    "    model.train() \n",
    "\n",
    "    for step, batch in enumerate(train_dataloader): \n",
    "        b_input_ids = batch[0].to(device) \n",
    "        b_labels    = batch[0].to(device)\n",
    "        b_masks     = batch[1].to(device) \n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model( input_ids = b_input_ids, labels = b_labels,\n",
    "                         attention_mask = b_masks, token_type_ids = None )\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Get sample every x batches.\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            model.eval()\n",
    "            print(infer(\"eggs, flour, butter, sugar\"))\n",
    "            model.train()\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202bad96-77d9-4fcb-91fb-7cd80409b1f8",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2e735d0-a4f8-4f0a-a51c-a1f69617d7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ingredients: eggs, mushroom, butter, sugar, milk, cream, cornstarch, salt, brown sugar, flour, baking powder, baking soda, baking soda.\n",
      "Instructions: In a large saucepan, bring eggs to boil. Reduce heat and simmer until thickened. Remove from heat and stir in mushroom, butter, sugar, milk, cornstarch, salt, and brown sugar. Add the flour, baking powder and baking soda; stir until well blended. Add the baking soda and stir until completely dissolved. Add milk and cornstarch mixture to the mushroom mixture. Stir well to combine. Divide mixture into 8 equal parts and bake at 350 degrees for 15 minutes. Let stand 5 minutes before serving. Makes 4–6 servings.\n"
     ]
    }
   ],
   "source": [
    "def infer(prompt):\n",
    "    input = f\"<|startoftext|>Ingredients: {prompt.strip()}\"\n",
    "    input = tokenizer(input, return_tensors=\"pt\")\n",
    "    input_ids      = input[\"input_ids\"]\n",
    "    attention_mask = input[\"attention_mask\"]\n",
    "\n",
    "    output = model.generate(input_ids.to(device),\n",
    "                            attention_mask=attention_mask.to(device),\n",
    "                            max_new_tokens=max_length,\n",
    "                            do_sample = True, top_k = 50, top_p = 0.85)\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return output\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_save_path)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_save_path)\n",
    "model.to(device)\n",
    "\n",
    "infer(\"eggs, mushroom, butter, sugar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2789990d-7792-44b9-8a0a-eb5433dca27a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
